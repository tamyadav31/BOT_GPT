# ============================================================================
# Database Configuration
# ============================================================================
# SQLite database URL. Use sqlite:///./bot_gpt.db for local development.
# For production, consider PostgreSQL: postgresql://user:password@host/dbname
DATABASE_URL=sqlite:///./bot_gpt.db

# ============================================================================
# LLM Configuration (OpenAI-compatible API)
# ============================================================================
# API Key for LLM service. Required for conversation functionality.
# Example: Groq API key from https://console.groq.com
LLM_API_KEY=your_groq_api_key_here

# Base URL for LLM API. Must be OpenAI-compatible.
# Groq: https://api.groq.com/openai
# OpenAI: https://api.openai.com/v1
LLM_API_BASE_URL=https://api.groq.com/openai

# Model name to use for chat completions.
# Groq models: mixtral-8x7b-32768, llama2-70b-4096, etc.
# OpenAI models: gpt-4, gpt-3.5-turbo, etc.
LLM_MODEL=mixtral-8x7b-32768

# Timeout in seconds for LLM API requests. Increase for slower networks.
LLM_TIMEOUT=30

# ============================================================================
# RAG Configuration (Retrieval-Augmented Generation)
# ============================================================================
# Embedding model for semantic search. Must be compatible with SentenceTransformers.
# Popular options:
# - sentence-transformers/all-MiniLM-L6-v2 (fast, good quality)
# - sentence-transformers/all-mpnet-base-v2 (slower, better quality)
RAG_MODEL=sentence-transformers/all-MiniLM-L6-v2

# Directory to store FAISS indices. Will be created if it doesn't exist.
RAG_INDEX_DIR=./data/indexes

# Number of top-k chunks to retrieve for RAG context.
# Higher values = more context but slower retrieval.
RAG_TOP_K=3

# ============================================================================
# Conversation Configuration
# ============================================================================
# Maximum number of previous messages to include in LLM context.
# Higher values = more context but higher token usage.
MAX_HISTORY_MESSAGES=10

# ============================================================================
# Document Chunking Configuration
# ============================================================================
# Size of each text chunk in characters.
# Larger chunks = more context but less granular retrieval.
CHUNK_SIZE=500

# Number of characters to overlap between chunks.
# Helps maintain context across chunk boundaries.
CHUNK_OVERLAP=50

# ============================================================================
# API Configuration
# ============================================================================
# Enable debug mode for development. Set to False in production.
DEBUG=False
